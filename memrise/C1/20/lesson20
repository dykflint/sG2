Signs – das neue Tool erleichtert die Kommunikation
Sprachassistenten gibt es seit knapp 10 Jahren im Überfluss, und jeder Smartphone-Nutzer kennt mindestens das, was er auf seinem Gerät hat. Als Software sind sie eine Erweiterung der Diktierprogramme, die früher in Büros Einsatz fanden. Während jedoch die früheren Programme nur „gehört“ und das Gehörte verschriftlicht haben, sind die modernen Assistenten in der Lage, Sprachbefehle mittels Sprachanalyse in Dienste umzusetzen, Informationen abzufragen und Dialoge zu führen. Der in der gesprochenen Sprache eingehende Befehl wird mithilfe verschiedener Algorithmen semantisch interpretiert, logisch verarbeitet und als Ergebnis formuliert bzw. eingesetzt, um eine weitere beabsichtigte Handlung zu vollziehen. Bei Unklarheiten werden Rückfragen gestellt oder es wird um eine Bestätigung gebeten. Es ist also ein rundum gutes und sinnvolles Produkt.
Nun haben die Sprachassistenten, wie Google Assistant, Siri, Alexa oder Cortana keinen Nutzen, solange die Person ihre Gedanken nicht laut aussprechen kann, wie alle Menschen, die in der  Gebärdensprache kommunizieren. Wie können Personen, die nicht laut sprechen können, solche Systeme nutzen? Diese Frage stand am Anfang der Überlegungen für die Mitarbeiter der Digitalagentur MRM//McCann, die auf den Einsatz neuer Technologien und die Trendforschung spezialisiert ist. Technisch stand das Team des Digitallabs LAB13 vor dem Problem, wie Sprachassistenten die Gebärdensprache in Befehle umsetzen könnten, um ein Tool zu entwickeln, das Hörbeeinträchtigten eine einwandfreie Kommunikation mit dem Sprachassistenten ermöglicht und sich darüber hinaus problemlos in den Alltag integrieren lässt. So wurde das Tool Signs geboren.
Der erste Schritt führte die Entwickler zu einer Lehrerin für Gebärdensprache, die aus ihrem Alltag über die Probleme berichtete und Ideen für die ersten Entwürfe lieferte. Mittels einer Kamera wurden in der ersten Phase diverse Bewegungen der Finger und Hände aufgezeichnet, um daraus allgemeine Strukturen abzuleiten, die typisch für bestimmte Gebärden sind, das sogenannte Tracking der Gebärden. Darauf folgte ein weiterer Prototyp, in den zusätzlich der gesamte Oberkörper der Person miteinbezogen wurde. Dank der Mitarbeit zahlreicher Freiwilliger entstand in kurzer Zeit ein Pool an aufgezeichneten Gebärden in einer Standardform. Diese bildeten eine Basis für die systematische Erfassung von Gesten, sowie deren Kategorisierung und Zuordnung. 
Die Idee selbst ist nicht kompliziert. Die Gebärden des Nutzers werden durch eine Kamera, wie jedes Endgerät – ein Laptop, ein Smartphone oder eine Uhr – sie hat, als Videostream aufgezeichnet, wobei das Ganze in Einzelbilder zerlegt und in das System gespeist wird. Signs gleicht die kommunizierten Gesten in Echtzeit mit der Wortdatenbank ab, die Anwendung wandelt den Textbefehl für den Assistenten in ein Audio um. Die Umwandlung des Textes in Audio ist notwendig, da die Schnittstellen der Voice Assistants meist nur den Ton verstehen. Die Antwort funktioniert umgekehrt: Das Audio wird mittels des Googles Cloud Speech-to-Text-Service in einen geschriebenen Text umgewandelt, der anschließend auf dem Bildschirm erscheint. 
Eine größere Schwierigkeit als die Entwicklung des Prinzips selbst war jedoch die Übertragung der Struktur der Gebärdensprache auf das gesprochene Standarddeutsch, mit dem der Sprachassistent arbeitet. Allem voran ist die Satzstellung in beiden Varianten nicht gleich, denn die Gebärdensprache verfügt über keine grammatikalisch gekennzeichneten Zeitformen und keine Füllwörter. So gesehen, ähnelt ihre Grundstruktur weitgehend der des Chinesischen und weist große Unterschiede zur deutschen Standardgrammatik auf. Für die Programmierung hatte das sowohl Vor- als auch Nachteile. Die Übersetzung der Gebärden in eine Wortvariante erschien einfach, denn aus der Frage „Wann habe ich meinen Arzttermin?“ blieb die Struktur „Arzt – Termin“ übrig, was als Befehl für die Suche völlig ausreicht. Damit das für den Sprachassistenten jedoch verständlich ist, musste ein Zwischenschritt eingebaut werden, den die Entwickler „ein kontextsensitives System“ nennen. Dieses bringt die Struktur, die aus den Gebärden dekodiert wurde, vor der Audio-Umwandlung in die grammatikalisch korrekte Form des gesprochenen Deutschen, indem es den Satz sinngemäß füllt und die Wörter in die richtige Reihenfolge stellt. Eine weitere Hürde bestand darin, dass einige Gebärden zwei Bedeutungen haben, die nur im Kontext zu verstehen sind. Auch dafür fand man eine Lösung: Indem das kontextsensitive System den Sinn des Satzes analysiert, wählt es aus eingespeisten Vorschlägen die beabsichtigte Bedeutung. 
Die rasche Entwicklung von Signs wäre nach Aussagen der Autoren ohne die praktische Mitarbeit vieler freiwilliger Hörgeschädigter vom Verband junger Menschen mit Hörbehinderung e.V. nicht möglich gewesen. Während der gesamten Zeit, insbesondere jedoch in der monatelangen Testphase 2019, kamen einige Male pro Woche junge Menschen mit Hörbehinderung in die Agentur und testeten mit dem Team die einzelnen Schritte, gaben aber auch wertvolle Tipps zur Umsetzung und Verfeinerung, zumal sie als technikaffine Generation über ein breites allgemeines IT-Wissen verfügen. Auf diesem Testtraining basiert die Datenbank, mit der Signs zurzeit arbeitet. Sie war Anfang 2020 bereits stabil genug, um der Öffentlichkeit zur Verfügung gestellt zu werden. Da das Projekt von Anfang an vor allem ideell war, versuchen die Entwickler, durch kommerzielle Projekte, wie die Zusammenarbeit mit Banken oder Versicherungen, für die Menschen mit Gehörminderung eine große Zielgruppe darstellen, gewinnbringend zu arbeiten, um sicherzustellen, dass die Anwendung kostenlos bleibt. Zahlreiche Institutionen und Großkunden haben bereits ihr Kaufinteresse bekundet, es kommen auch vermehrt Anfragen von Verbänden aus dem Ausland. Für die Agentur ist der Markt aus finanziellen Gründen sehr interessant, weltweit leben nämlich knapp 470 Millionen Gehörbeeinträchtigte, und die großen Konzerne haben diese Zielgruppe bisher komplett vernachlässigt. (841 W.)




